{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/SocialComplexityLab/socialgraphs2021/wiki/Assignments) carefully before proceeding. This page contains information about formatting (including formats etc.), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Tuesday October the 31st, 2021 at 23:55. Hand in your IPython notebook file (with extension `.ipynb`) via http://peergrade.io/\n",
    "\n",
    "\n",
    "(If you haven't set up an account on peergrade yet, go to www.peergrade.io/join and type in the class code: ***DPZEV6***.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This year's Assignment 2 is all about analyzing the network of rappers.\n",
    "\n",
    "Note that this time I'm doing the exercises slightly differently in order to clean things up a bit. The issue is that the weekly lectures/exercises include quite a few instructions and intermediate results that are not quite something you guys can meaningfully answer. \n",
    "\n",
    "Therefore, in the assignment below, I have tried to reformulate the questions from the weekly exercises into something that is (hopefully) easier to answer. *Then I also note which lectures each question comes from*; that way, you can easily go back and find additional tips & tricks on how to solve things ðŸ˜‡\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Building the network \n",
    "\n",
    "To create our network, we downloaded the rapper Wiki pages from each coast (during Week 4) and linked them via the hyperlinks connecting pages to each other. To achieve this goal we have used regular expressions.\n",
    "\n",
    "> * Explain the strategy you have used to extract the hyperlinks from the Wiki-pages, assuming that you have already collected the rapper pages with the Wikipedia API.\n",
    "> * Show the regular expressions you have built and explain in details how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the Wiki links from the rapper pages loop over the wiki page of each rapper. Here the page of the current rapper is denoted ``` text ``` . In this we know to look for the pattern ```[[link]]``` or ```[[link | alias]]```. Therefore we have used a regular expression in the following way: \n",
    "\n",
    "```python\n",
    "pattern = r\"\\[\\[(.+?)\\]\\]\"\n",
    "links = re.findall(pattern, text)\n",
    "```\n",
    "\n",
    "Here the ``findall`` function ensures that all instances of matches are found. The pattern is defined as follows:\n",
    "\n",
    "* ``\\[\\[`` : The pattern starts with two square brackets\n",
    "* ```(.+?)``` : The pattern group (denoted with ``()``) continues with any character (```.```) between one and unlimited times (```+```). Finally the question mark ensures that the pattern is \"lazy\".\n",
    "* ```\\]\\]``` : The pattern ends with two square brackets\n",
    "\n",
    "Finally we add the following line to remove the alias from the links:\n",
    "    \n",
    "```python\n",
    "links = [link.split('|')[0] for link in links]\n",
    "```\n",
    "Hereby a list of links is created for each rapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Network visualization and basic stats\n",
    "\n",
    "Visualize your network of rappers (from lecture 5) and calculate stats (from lecture 4 and 5). For this exercise, we assume that you have already generated the network and extracted the largest weakly connected component (the \"largest weakly connected component\" of a directed network is the subgraph consisting of the nodes that would constitute the largest connected component if the network were undirected) . The visualization and statistics should be done for the largest weakly connected component only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the graph\n",
    "G = nx.read_gexf('rapper_network.gexf')\n",
    "\n",
    "# extract largest weakly connected component\n",
    "largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "g = G.subgraph(largest_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 1a_: Stats (see lecture 4 and 5 for more hints)\n",
    "\n",
    "> * What is the number of nodes in the network? \n",
    "> * What is the number of links?\n",
    "> * Who is the top connected rapper? (Report results for the in-degrees and out-degrees). Comment on your findings. Is this what you would have expected?\n",
    "> * Who are the top 5 most connected east-coast rappers (again in terms of in/out-degree)? \n",
    "> * Who are the top 5 most connected west-coast rappers (again in terms of in/out-degree)?\n",
    "> * Plot the in- and out-degree distributions for the whole network. \n",
    ">   * Use axes that make sense for visualizing this particular distribution.\n",
    ">   * What do you observe? \n",
    ">   * Give a pedagogical explaination of why the in-degree distribution is different from the out-degree distribution?\n",
    "> * Find the exponent (by using the `powerlaw` package) for the in- and out-degree distributions. What does it say about our network?\n",
    "> * Compare the two degree distributions two the degree distribution of a *random network* (undirected) with the same number of nodes and probability of connection *p*. Comment your results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:  728\n",
      "Number of links:  5824\n"
     ]
    }
   ],
   "source": [
    "# print the number of nodes and edges\n",
    "print(\"Number of nodes: \", g.number_of_nodes())\n",
    "print(\"Number of links: \", g.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest in-degree: Snoop Dogg , 125\n",
      "Highest out-degree: Drag-On , 52\n"
     ]
    }
   ],
   "source": [
    "# find most connected rapper\n",
    "in_degree_dict = dict(g.in_degree(g.nodes()))\n",
    "out_degree_dict = dict(g.out_degree(g.nodes()))\n",
    "\n",
    "# sort the dictionary by degree\n",
    "print(\"Highest in-degree:\", max(in_degree_dict, key=in_degree_dict.get), \",\", in_degree_dict[max(in_degree_dict, key=in_degree_dict.get)])\n",
    "print(\"Highest out-degree:\", max(out_degree_dict, key=out_degree_dict.get), \",\", out_degree_dict[max(out_degree_dict, key=out_degree_dict.get)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rapper who is the most referenced by other rappers wikitext is Snoop Dog. Snoop Dog is a well known rapper with a large discography. On the other hand Drag-On has the most references to other rappers. Drag-On is not as well know....... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 1b_: Visualization (see lecture 5 for more hints)\n",
    "\n",
    "> * Create a nice visualization of the total (directed) network:\n",
    ">   * Color nodes according to the role;\n",
    ">   * Scale node-size according to degree;\n",
    ">   * Get node positions based on either the Force Atlas 2 algorithm, or the built-in algorithms for networkX;\n",
    ">   * Whatever else you feel like that would make the visualization nicer.\n",
    "> * Describe the structure you observe. What useful information can you decipher from this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Word-clouds\n",
    "\n",
    "Create your own version of the word-clouds (from lecture 7). For this exercise we assume you know how to download and clean text from rappers' Wikipedia pages.\n",
    "\n",
    "Here's what you need to do:\n",
    "> * Create a word-cloud for each coast according to the novel TF-TR method. Feel free to make it as fancy as you like. Explain your process and comment on your results.\n",
    "> * For each coast, what are the 5 words with the highest TR scores? Comment on your result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Communities\n",
    "\n",
    "Find communities and their modularity (from lecture 7).\n",
    "\n",
    "Here's what you need to do:\n",
    "> * In your own words, explain what the measure \"modularity\" is, and the intuition behind the formula you use to compute it. \n",
    "> * Find communities in the network, and explain how you chose to identify the communities: Which algorithm did you use and how does it work?\n",
    "> * Comment on your results:\n",
    ">   * How many communities did you find in total?\n",
    ">   * Compute the value of modularity with the partition created by the algorithm.\n",
    ">   * Plot and/or print the distribution of community sizes (whichever makes most sense). Comment on your result.\n",
    "> * Now, partition your rappers into two communities based on which coast they represent.\n",
    ">   * What is the modularity of this partition? Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Sentiment of communities\n",
    "\n",
    "Analyze the sentiment of communities (lecture 8). More tips & tricks can be found, if you take a look at Lecture 8's exercises.\n",
    "\n",
    "A couple of additional instructions you will need below:\n",
    "* Average the average sentiment of the nodes in each community to find a community-level sentiment.\n",
    "\n",
    "Here's what you need to do (use the LabMT wordlist approach):\n",
    "> * Calculate and store sentiment for every rapper\n",
    "> * Create a histogram of all rappers' associated sentiments.\n",
    "> * What are the 10 rappers with happiest and saddest pages?\n",
    "\n",
    "Now, compute the sentiment of each coast: \n",
    "> * Which is the happiest and which is saddest coast according to the LabMT wordlist approach? (Take the coast's sentiment to be the average sentiment of the coast's rappers' pages (disregarding any rappers with sentiment 0).\n",
    "> * Use the \"label shuffling test\" (Week 5 and 8) to test if the coast with the highest wikipedia page sentiment has a page sentiment that is significantly higher (5% confidence bound) than a randomly selected group of rappers of the same size.\n",
    "> * Does the result make sense to you? Elaborate.\n",
    "\n",
    "**Congratulations for making it to the end of the Assignment. Good luck with your independent project**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
